{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SocSysSci/B5_TextProcessing2/blob/main/B5_TextAnalysis2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. テキストの収集\n",
        "\n",
        "- 青空文庫からテキストを収集します。\n",
        "- まず，補足資料の「文書の収集」の箇所を読み、作業を行って下さい。\n",
        "- その後，説明を読みながら各セルを実行して下さい。"
      ],
      "metadata": {
        "id": "vNOQnhHdBTw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. 【授業中は実行しません】 スクレイピングによるテキストの収集\n",
        "\n",
        "**すごく時間がかかるので授業中はここを実行せずに収集済みのテキストデータを利用します！！！**\n",
        "\n",
        "ここでは「青空文庫」からデータを取得します。以下のように実行すると指定された著者の指定された著作のデータを取得します。\n",
        "\n",
        "```\n",
        "text = get_authors_work(\"著者名\", \"作品名\")\n",
        "```\n",
        "\n",
        "著者名と作品名は**青空文庫の索引にある通り**に指定する必要があります。例えば，著者名の姓と名の間には半角スペースを入れます。"
      ],
      "metadata": {
        "id": "cJgVYzY_agmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なパッケージの読み込み\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re"
      ],
      "metadata": {
        "id": "wsK0YW3nbCej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j9Rz-RB7uPx"
      },
      "outputs": [],
      "source": [
        "# スクレイピングを行う関数の定義\n",
        "\n",
        "def get_author_page_url(author):\n",
        "    base_url = \"https://www.aozora.gr.jp/index_pages/\"\n",
        "    res = requests.get(base_url + \"person_all.html\")\n",
        "    soup = BeautifulSoup(res.text.encode(res.encoding), \"html.parser\")\n",
        "    author_link = soup.find(\"a\", text=author)\n",
        "    href = None\n",
        "    if author_link is not None:\n",
        "        href = base_url + author_link.get(\"href\").split(\"#\")[0]\n",
        "    return href\n",
        "\n",
        "\n",
        "def get_author_works_urls(author):\n",
        "    base_url = \"https://www.aozora.gr.jp/\"\n",
        "    url_regex = re.compile(r\"cards\\/.+\\.html$\")\n",
        "\n",
        "    author_url = get_author_page_url(author)\n",
        "    works_urls = {}\n",
        "    if author_url is not None:\n",
        "        res = requests.get(author_url)\n",
        "        soup = BeautifulSoup(res.text.encode(res.encoding), \"html.parser\")\n",
        "        works_links = soup.find_all(\"a\", {\"href\": url_regex})\n",
        "        if works_links is not None:\n",
        "            for w in works_links:\n",
        "                w_title = w.text\n",
        "                w_url = base_url + url_regex.search(w.get(\"href\")).group()\n",
        "                works_urls[w_title] = w_url\n",
        "    return works_urls\n",
        "\n",
        "\n",
        "def get_author_work_page_url(author, work):\n",
        "    works_urls = get_author_works_urls(author)\n",
        "    url = None\n",
        "    if work in works_urls.keys():\n",
        "        url = works_urls[work]\n",
        "    return url\n",
        "\n",
        "\n",
        "def get_author_work_url(author, work):\n",
        "    work_page_url = get_author_work_page_url(author, work)\n",
        "    url = None\n",
        "    if work_page_url is not None:\n",
        "        res = requests.get(work_page_url)\n",
        "        soup = BeautifulSoup(res.text.encode(res.encoding), \"html.parser\")\n",
        "        dl_tag = soup.find(\"table\", class_=\"download\")\n",
        "        if dl_tag is not None:\n",
        "            xhtml_tag = dl_tag.find(\"a\", {\"href\": re.compile(r\"\\.html$\")})\n",
        "            if xhtml_tag is not None:\n",
        "                url = xhtml_tag.get(\"href\")\n",
        "    if url is not None:\n",
        "        url = os.path.dirname(work_page_url) + \"/files/\" + os.path.basename(url)\n",
        "    return url\n",
        "\n",
        "\n",
        "def get_author_work_content(author, work):\n",
        "    work_url = get_author_work_url(author, work)\n",
        "    content = None\n",
        "    if work_url is not None:\n",
        "        res = requests.get(work_url)\n",
        "        soup = BeautifulSoup(res.text.encode(res.encoding), \"html.parser\")\n",
        "        content_tag = soup.find(\"div\", class_=\"main_text\")\n",
        "        if content_tag is not None:\n",
        "          content = content_tag.decode_contents()\n",
        "    return content\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 誤って実行しないようにコメントアウトしています。実行する場合は2行目と10行目を削除します。\n",
        "\"\"\"\n",
        "text1 = get_author_work_content(\"芥川 竜之介\", \"河童\")\n",
        "text2 = get_author_work_content(\"芥川 竜之介\", \"地獄変\")\n",
        "text3 = get_author_work_content(\"太宰 治\", \"人間失格\")\n",
        "text4 = get_author_work_content(\"太宰 治\", \"斜陽\")\n",
        "text5 = get_author_work_content(\"夏目 漱石\", \"吾輩は猫である\")\n",
        "text6 = get_author_work_content(\"夏目 漱石\", \"坊っちゃん\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UMaRf8FyBSAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. 【授業時間はこちらを実施】テキストの読み込み\n",
        "\n",
        "演習で全員が一斉にアクセスすると青空文庫に過大な負荷がかかる可能性があるので，演習では，予め取得しておいたテキストをアップロードして利用します。\n",
        "\n",
        "B-3の資料を参照しながら，BEEF+からテキストデータをダウンロードし，Google Colaboratoryのセッションストレージにアップロードしてから以下のセルを実行してください。"
      ],
      "metadata": {
        "id": "rnLtYdnnXcqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# テキスト読み込み用の関数定義\n",
        "def file_load(file_name):\n",
        "  text = None\n",
        "  with open(file_name, \"r\") as f:\n",
        "    text = \"\\n\".join(f.readlines())\n",
        "  return text"
      ],
      "metadata": {
        "id": "JU5I2NIVbP4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = file_load(\"akage.html\")     # 赤毛連盟\n",
        "text2 = file_load(\"akiya.html\")     # 空き家の冒険\n",
        "text3 = file_load(\"odoru.html\")     # 踊る人形\n",
        "text4 = file_load(\"madara.html\")    # まだらの紐\n",
        "text5 = file_load(\"bohemia.html\")   # ボヘミアの醜聞\n",
        "text6 = file_load(\"saigo.html\")     # 最後の事件"
      ],
      "metadata": {
        "id": "8mA2IjSubLbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. テキストの確認\n",
        "\n",
        "取得したテキストを確認します。先頭の一部分が表示されます。何も表示されない場合は，うまくテキストが取得できなかった可能性があります。ここまでの手順を見直して下さい。\n",
        "\n",
        "ここでは text1（赤毛連盟）を確認しています。"
      ],
      "metadata": {
        "id": "SK1f7MU2HXCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1"
      ],
      "metadata": {
        "id": "7Pc4P5KXH96l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. クレンジング\n",
        "\n",
        "テキストに含まれる不要なデータを取り除きます。具体的には以下のデータを取り除きます。\n",
        "- 余分な余白\n",
        "- ルビ\n",
        "- 改行以外のタグ"
      ],
      "metadata": {
        "id": "p2CFDw9AKz_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1. クレンジング用の関数定義\n",
        "\n",
        "クレンジングを実際に行う関数を定義します。このセルを実行すると，そのあとに実行されるコードセルから，定義された関数（defで定義されているブロック）を呼び出すことができます。\n",
        "\n",
        "なお，プログラムは順番に実行されることに注意しましょう。1番目の処理と2番目の処理を入れ替えて，「`<br/>` タグを改行に変換」を先に実行すると，変換された改行が除去されてしまいます。"
      ],
      "metadata": {
        "id": "o4Cbc6tNKrIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def cleansing(text):\n",
        "  clean_text = re.sub(\"\\s\", \"\", text)                     # 余分な空白（改行や字下げの空白）を除去\n",
        "  clean_text = clean_text.replace(\"<br/>\", \"\\n\")          # <br/>タグを改行に変換\n",
        "  clean_text = re.sub(r\"<rp>[^<]+</rp>\", \"\", clean_text)  # ルビの前後の括弧を除去\n",
        "  clean_text = re.sub(r\"<rt>[^<]+</rt>\", \"\", clean_text)  # ルビのテキストを除去\n",
        "  clean_text = re.sub(r\"<[^>]+>\", \"\", clean_text)         # それ以外のタグを除去\n",
        "  return clean_text"
      ],
      "metadata": {
        "id": "ZHaYgyasKzr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-2. クレンジングの実行\n",
        "\n",
        "不要なデータを取り除きます。"
      ],
      "metadata": {
        "id": "ZYY1EJEy-mMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text1 = cleansing(text1)\n",
        "clean_text2 = cleansing(text2)\n",
        "clean_text3 = cleansing(text3)\n",
        "clean_text4 = cleansing(text4)\n",
        "clean_text5 = cleansing(text5)\n",
        "clean_text6 = cleansing(text6)"
      ],
      "metadata": {
        "id": "C1SYEc8NK3Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-3. クレンジングの結果を確認\n",
        "\n",
        "クレンジング後のデータを確認します。"
      ],
      "metadata": {
        "id": "ugbHOa7AUswm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text1"
      ],
      "metadata": {
        "id": "w9Q4igIzUvu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. トークン化\n",
        "\n",
        "以下では，形態素解析パッケージを利用して，テキストデータをトークン（ここでは形態素）に分割，分かち書きのテキストに変換します。その際，分析に必要のある品詞のトークンのみを残すようにします。"
      ],
      "metadata": {
        "id": "vdM6IfJmazLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-1. 形態素解析パッケージのインストール\n",
        "\n",
        "形態素解析パッケージ Janome をインストールします。あとのコードセルを実行する前に1度だけ実行すれば大丈夫です。"
      ],
      "metadata": {
        "id": "ZjaSdugmWacQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome"
      ],
      "metadata": {
        "id": "azxuYB0WWfW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-2. トークン化（分かち書き）用の関数定義\n",
        "\n",
        "トークン化を実際に行う関数を定義します。これもあとのコードの実行前に1度だけ実行すれば大丈夫です。このセルを実行すると，そのあとに実行されるコードセルから，定義された関数（defで定義されているブロック）を呼び出すことができます。"
      ],
      "metadata": {
        "id": "gdzOO8pWbVvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "\n",
        "def wakati_text(text, pos=[\"名詞\", \"動詞\"]):\n",
        "    tokenizer = Tokenizer()\n",
        "    doc = tokenizer.tokenize(text)\n",
        "    wakati = None\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        p = token.part_of_speech.split(\",\")[0]\n",
        "        if p in pos:\n",
        "            word_list.append(token.base_form)\n",
        "    if 0 < len(word_list):\n",
        "        wakati = \" \".join(word_list)\n",
        "    return wakati"
      ],
      "metadata": {
        "id": "pqP9aNPpbVAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-3. トークン化する\n",
        "\n",
        "以下のコードを実行してみます。（Janomeは遅いんで結構時間かかります）\n",
        "\n",
        "この時，助詞など利用頻度が高いが分析ではあまり重要でないことが明らかなトークンについては，以降の分析等で利用しないために，利用する品詞の指定をしています。"
      ],
      "metadata": {
        "id": "7l9R-On8cJpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wakati1 = wakati_text(clean_text1, [\"名詞\", \"形容動詞\", \"形容詞\", \"動詞\"])\n",
        "wakati2 = wakati_text(clean_text2, [\"名詞\", \"形容動詞\", \"形容詞\", \"動詞\"])\n",
        "wakati3 = wakati_text(clean_text3, [\"名詞\", \"形容動詞\", \"形容詞\", \"動詞\"])\n",
        "wakati4 = wakati_text(clean_text4, [\"名詞\", \"形容動詞\", \"形容詞\", \"動詞\"])\n",
        "wakati5 = wakati_text(clean_text5, [\"名詞\", \"形容動詞\", \"形容詞\", \"動詞\"])\n",
        "wakati6 = wakati_text(clean_text6, [\"名詞\", \"形容動詞\", \"形容詞\", \"動詞\"])"
      ],
      "metadata": {
        "id": "FWgzanKrcWEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-4. トークン化の確認\n",
        "\n",
        "トークン化されたテキストを確認します。"
      ],
      "metadata": {
        "id": "CuNZa_FMcdrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wakati1"
      ],
      "metadata": {
        "id": "jqeAtFhRclPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 文書ごとのベクトル化\n",
        "\n",
        "分かち書きされたテキストをベクトル化していきます。"
      ],
      "metadata": {
        "id": "c1nEBl0bG6A0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-1. テキストデータの結合\n",
        "\n",
        "複数の**分かち書きされたテキストデータ**を1つのリストにまとめます。演習では6つのテキストデータ（ wakati1, wakati2, wakati3, wakati4, wakati5, wakati6）をまとめます。"
      ],
      "metadata": {
        "id": "Eaj0cvxMQkKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [wakati1, wakati2, wakati3, wakati4, wakati5, wakati6]"
      ],
      "metadata": {
        "id": "9H8wziUjQs8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-2. BoW (Bag-of-Words) の作成\n"
      ],
      "metadata": {
        "id": "-yALYHiZILG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "counter = CountVectorizer()\n",
        "bow = counter.fit_transform(docs)"
      ],
      "metadata": {
        "id": "DOKvdZIYH3Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-3. BoWの確認\n",
        "\n",
        "BoWを確認します。結果の見方については補足資料の「BoW」のページを見て下さい。"
      ],
      "metadata": {
        "id": "MWYTlCOCRFRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow.toarray()"
      ],
      "metadata": {
        "id": "dpe43141RK-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-4. TF-IDFの作成"
      ],
      "metadata": {
        "id": "Z3nHWUOtUuzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "trans = TfidfTransformer()\n",
        "tfidf = trans.fit_transform(bow)"
      ],
      "metadata": {
        "id": "rTuRsI-4UzY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-5. TF-IDFの確認\n",
        "\n",
        "TF-IDFを確認します。結果の見方については補足資料の「TF-IDF」のページを見て下さい。"
      ],
      "metadata": {
        "id": "NpR6EgO9VBl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.toarray()"
      ],
      "metadata": {
        "id": "CEVas2wcVAs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 可視化\n",
        "\n",
        "ここではそれぞれのテキストの概要をつかむために，可視化を行ってみます。具体的には Word Cloud の作成を行ってみます。"
      ],
      "metadata": {
        "id": "fT_pHsl0fpn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-1. 日本語フォントのインストール\n",
        "\n",
        "Google Colaboratoryのサーバには日本語フォントが入っていないため，日本語が表示されません。そこで，まず，日本語フォントをインストールします。これは以下のセルの実行前に一度実行すれば大丈夫です。"
      ],
      "metadata": {
        "id": "VQqnAYXfgjSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install fonts-noto-cjk"
      ],
      "metadata": {
        "id": "baQtq2sKfzFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-2. マスク画像のアップロード\n",
        "\n",
        "WordCloudの部分は黒，背景は白となるマスク画像を用いると，任意の形状のWordCloud画像が生成できます。\n",
        "\n",
        "ここでは楕円形の画像（oval_bw.png）を用いて，楕円形のWordCloud画像を作ってみます。\n",
        "\n",
        "BEEF+から oval_bw.png をダウンロードし，Google Colaboratoryのセッションストレージにアップロードしてください。"
      ],
      "metadata": {
        "id": "775-5-abfWxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なパッケージの読み込み\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "xx7I8ngUgI9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# マスク画像の読み込み\n",
        "mask_image = np.array(Image.open(\"oval_bw.png\"))"
      ],
      "metadata": {
        "id": "lGsYaHJPgSHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-2. Word Cloudオブジェクトの用意\n",
        "\n",
        "Word Cloudの大きさや背景色，ストップワードなどをここで指定します。ストップワードとは不要な語のことで，ここではWord Cloudに含めない語のことです。\n",
        "\n",
        "オブジェクト生成の際に，作成する画像の大きさなどを指定しています。この説明については資料の「WordCloud」のページを参照してください。"
      ],
      "metadata": {
        "id": "-wP-gUuph5xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wordcloud = WordCloud(font_path=\"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\",\n",
        "                      background_color = \"white\",\n",
        "                      width = 1600, height = 900,\n",
        "                      mask = mask_image)"
      ],
      "metadata": {
        "id": "T2UgCGzkgeHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-3. Word Cloud用データの作成\n",
        "\n",
        "上で計算したベクトルデータを用いて，Word Cloud 用のデータを作成します。ここでは BoW を用いていますが，，TF-IDFを用いて Word Cloud を作成することも可能です。"
      ],
      "metadata": {
        "id": "nRj6OrnzjBvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wc_data = pd.DataFrame(tfidf.toarray(),\n",
        "                       columns = counter.get_feature_names_out())"
      ],
      "metadata": {
        "id": "5m1YH-2PmUYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5-4. Word Cloudの作成\n",
        "\n",
        "ここでは1番目の文書（赤毛連盟）についてWord Cloudを作成しています。それ以外の文書のWord Cloudを作成する場合は補足資料を参考にして下さい。"
      ],
      "metadata": {
        "id": "W6OKhfhBma2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = [\"ワトソン\", \"ベーカー街\"]  # ストップワード（Word Cloudに含めない単語）\n",
        "words = wc_data.loc[0].to_dict()     # 0番目のデータ（ここでは「赤毛連盟」）の WordCloud を作成\n",
        "for sw in stopwords:\n",
        "  words.pop(sw, None)\n",
        "img = wordcloud.fit_words(words)\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "g38a_eHKi0NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. 分析\n",
        "\n",
        "分析として，テキスト間の距離や類似度などを計算することも多いですが，ここでは主成分分析（PCA:Principal Component Analysis）を利用して，テキストデータ間の距離を可視化してみます。これによりテキスト間の距離が直感的に把握できます。"
      ],
      "metadata": {
        "id": "d7YhqMI5oPLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-1. 日本語をプロットできるようにするパッケージのインストール"
      ],
      "metadata": {
        "id": "sv24CzKcuNnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install japanize-matplotlib"
      ],
      "metadata": {
        "id": "4S-lVrmkuTfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-2. 主成分分析の実行"
      ],
      "metadata": {
        "id": "LaG015QnuevL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components = 0.9, whiten = False)\n",
        "pca.fit(bow.toarray())\n",
        "x = pca.transform(bow.toarray())"
      ],
      "metadata": {
        "id": "aEFUfiwtvAhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "主成分の数を確認します。"
      ],
      "metadata": {
        "id": "NmA1_zHMa4ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.n_components_"
      ],
      "metadata": {
        "id": "feo_1LB5a7nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-3. 寄与率の確認\n",
        "\n",
        "寄与率は，各主成分がデータ表現にどれぐらい寄与しているかを示します。簡単にいうと主成分の重要度と言っても良いです。第1主成分が最も重要で，第2，第3と続きます。"
      ],
      "metadata": {
        "id": "-_dbiwhQbAm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "gJ8vwJ1GbUNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-4. 主成分空間でのテキストのプロット\n",
        "\n",
        "この部分の説明については補足資料を参照して下さい。"
      ],
      "metadata": {
        "id": "ufI-4Kxou5YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import japanize_matplotlib\n",
        "\n",
        "name = [\"赤毛連盟\", \"空き家の冒険\", \"踊る人形\", \"まだらの紐\", \"ボヘミアの醜聞\", \"最後の事件\"]\n",
        "for i in range(len(name)):\n",
        "  plt.scatter(x[i, 0], x[i, 1], label=name[i])\n",
        "  plt.text(x[i, 0], x[i, 1], name[i])"
      ],
      "metadata": {
        "id": "a6uxRIyboiva"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}